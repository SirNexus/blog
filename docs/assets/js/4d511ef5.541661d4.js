"use strict";(self.webpackChunkaidan_blog=self.webpackChunkaidan_blog||[]).push([[888],{528:e=>{e.exports=JSON.parse('{"permalink":"/blog/istio-pod-to-pod-mtls","source":"@site/blog/2025-04-29-istio-pod-to-pod-mtls.md","title":"Enabling pod to pod mTLS in Istio","description":"Imagine a situation where you have multiple different clusters, each running","date":"2025-04-29T00:00:00.000Z","tags":[{"inline":true,"label":"Istio","permalink":"/blog/tags/istio"},{"inline":true,"label":"ServiceEntry","permalink":"/blog/tags/service-entry"},{"inline":true,"label":"pod","permalink":"/blog/tags/pod"},{"inline":true,"label":"mTLS","permalink":"/blog/tags/m-tls"},{"inline":true,"label":"strict","permalink":"/blog/tags/strict"},{"inline":true,"label":"authentication","permalink":"/blog/tags/authentication"}],"readingTime":13.73,"hasTruncateMarker":true,"authors":[{"name":"Aidan Carson","key":"aidancarson","page":null}],"frontMatter":{"slug":"istio-pod-to-pod-mtls","title":"Enabling pod to pod mTLS in Istio","authors":"aidancarson","tags":["Istio","ServiceEntry","pod","mTLS","strict","authentication"],"toc_min_heading_level":2,"toc_max_heading_level":5},"unlisted":false,"prevItem":{"title":"Networking Multiple Kind Kubernetes Clusters Together Using Native Routing","permalink":"/blog/kind-multi-cluster-flat-network"}}')},4392:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>l});var o=t(528),r=t(4848),c=t(8453);const s={slug:"istio-pod-to-pod-mtls",title:"Enabling pod to pod mTLS in Istio",authors:"aidancarson",tags:["Istio","ServiceEntry","pod","mTLS","strict","authentication"],toc_min_heading_level:2,toc_max_heading_level:5},a="Enabling pod to pod mTLS in Istio",i={authorsImageUrls:[void 0]},l=[{value:"Context",id:"context",level:2},{value:"Setup",id:"setup",level:2},{value:"The Problem",id:"the-problem",level:2},{value:"Finding the Solution",id:"finding-the-solution",level:2},{value:"Summary",id:"summary",level:2},{value:"Related Issues",id:"related-issues",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,c.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Imagine a situation where you have multiple different clusters, each running\nan Istio service mesh and which are federated together to talk to each other.\nNow also imagine that these clusters are networked together such that each\npod IP is uniquely addressable and able to be communicated with from any other\ncluster."}),"\n",(0,r.jsx)(n.h2,{id:"context",children:"Context"}),"\n",(0,r.jsx)(n.p,{children:"I faced a situation like this, where I needed to group endpoints into logical\nhostnames that represented services backed by those endpoints. Because endpoints\ncould live anywhere, in any cluster, I landed on using a ServiceEntry to register\nthe hostname and WorkloadEntries to represent the endpoints service that hostname."}),"\n",(0,r.jsx)(n.p,{children:"But a problem came on enabling PeerAuthentication:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: security.istio.io/v1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: prod-istio-system # The namespace of our istio installation\nspec:\n  mtls:\n    mode: STRICT\n"})}),"\n",(0,r.jsx)(n.p,{children:"Curl stopped working!"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ curl global-echo\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\n"})}),"\n",(0,r.jsx)(n.p,{children:"Through some debugging, I was able to get it to:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ curl global-echo\nupstream connect error or disconnect/reset before headers. retried and the latest reset reason: remote connection failure, \ntransport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\n"})}),"\n",(0,r.jsx)(n.p,{children:"But to find the true root cause and solution took some digging."}),"\n",(0,r.jsx)(n.p,{children:"Let's get into it"}),"\n",(0,r.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,r.jsx)(n.p,{children:"The configuration looked something like this:"}),"\n",(0,r.jsx)(n.p,{children:"ServiceEntry:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nitems:\n  - apiVersion: networking.istio.io/v1\n    kind: ServiceEntry\n    metadata:\n      creationTimestamp: "2025-04-29T17:01:04Z"\n      generation: 7\n      name: global-curl-global-echo\n      namespace: curl\n      resourceVersion: "7068"\n      uid: 9c26a814-e852-4903-81a9-9213987236a0\n    spec:\n      hosts:\n        - global-echo\n      location: MESH_INTERNAL\n      ports:\n        - name: http\n          number: 80\n          protocol: HTTP\n          targetPort: 8080\n      resolution: STATIC\n      subjectAltNames:\n        - spiffe://cluster.local/ns/echo/sa/default\n        - spiffe://cluster.local/ns/echo/sa/default\n      workloadSelector:\n        labels:\n          global-service: global-echo\n    status:\n      addresses:\n        - host: global-echo\n          value: 240.240.0.1\n        - host: global-echo\n          value: 2001:2::1\nkind: List\nmetadata:\n  resourceVersion: ""\n'})}),"\n",(0,r.jsx)(n.p,{children:"WorkloadEntry:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"NAME                              AGE     ADDRESS\nglobal-echo-cluster1-10.4.2.45    4m58s   10.4.2.45\nglobal-echo-cluster2-10.6.1.212   27m     10.6.1.212\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nitems:\n  - apiVersion: networking.istio.io/v1\n    kind: WorkloadEntry\n    metadata:\n      creationTimestamp: "2025-04-29T17:24:24Z"\n      generation: 2\n      labels:\n        global-service: global-echo\n      name: global-echo-cluster1-10.4.2.45\n      namespace: curl\n      resourceVersion: "7188"\n      uid: d8c0a5da-aab8-4338-b890-b597a7883f47\n    spec:\n      address: 10.4.2.45\n      labels:\n        global-service: global-echo\n  - apiVersion: networking.istio.io/v1\n    kind: WorkloadEntry\n    metadata:\n      creationTimestamp: "2025-04-29T17:01:43Z"\n      generation: 4\n      labels:\n        global-service: global-echo\n      name: global-echo-cluster2-10.6.1.212\n      namespace: curl\n      resourceVersion: "7189"\n      uid: 72ff1595-e098-4eb2-9f84-f251d3c8faf4\n    spec:\n      address: 10.6.1.212\n      labels:\n        global-service: global-echo\nkind: List\nmetadata:\n  resourceVersion: ""\n\n'})}),"\n",(0,r.jsxs)(n.p,{children:["These resources specify a host ",(0,r.jsx)(n.code,{children:"global-echo"}),", and two backend services each of which\nlive in their own clusters. Behind the scenes, I have these services running simple\necho servers. If you'd like that yaml, here it is:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\n  namespace: echo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      containers:\n        - name: echo\n          image: hashicorp/http-echo\n          args:\n            - -text={{ .CurCluster }}\n            - -listen=:8080\n          resources:\n            requests:\n              cpu: "100m"\n              memory: "100Mi"\n            limits:\n              cpu: "100m"\n              memory: "100Mi"\n'})}),"\n",(0,r.jsx)(n.p,{children:"And it works! I can curl both pods in my mesh:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"for i in {1..10}; do curl global-echo; done\ncluster1\ncluster1\ncluster1\ncluster1\ncluster2\ncluster2\ncluster2\ncluster1\ncluster1\ncluster2\n"})}),"\n",(0,r.jsx)(n.h2,{id:"the-problem",children:"The Problem"}),"\n",(0,r.jsx)(n.p,{children:"So networking is working. Istio is generating the correct Envoy config such that\nour global hostname routes to our backing services on each cluster. But what about\nsecurity? Just to make sure that Istio is actually performing mTLS between pods,\nlet's turn on strict mTLS."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ kubectl apply -f - <<EOF\napiVersion: security.istio.io/v1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: prod-istio-system # The namespace of our istio installation\nspec:\n  mtls:\n    mode: STRICT\nEOF\n"})}),"\n",(0,r.jsx)(n.p,{children:"Let's check that our communication still works:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ for i in {1..10}; do curl global-echo; done\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\n"})}),"\n",(0,r.jsx)(n.p,{children:"Our connection broke! What gives! I thought Istio was routing our services? Spoiler:\nour connection broke because istio was using PERMISSIVE mode and sending our traffic\nover plaintext. Now that we are enforcing STRICT mTLS mode, Istio is breaking.\nThis means that we weren't secure before, sending traffic over plaintext. This is\nsuper interesting. Istio should be handling the upgrading of our communication to\nmTLS by default. So what's happening? Let's do some testing."}),"\n",(0,r.jsx)(n.h2,{id:"finding-the-solution",children:"Finding the Solution"}),"\n",(0,r.jsxs)(n.p,{children:["So let's look at the local endpoint. Curling from cluster1, we have a local echo\npod IP of ",(0,r.jsx)(n.code,{children:"10.4.2.45"}),". Let's curl that:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ curl 10.4.2.45\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Hm, so that's broken. That makes sense, since the service entry is just resolving\nDNS requests to the WorkloadEntry specifying ",(0,r.jsx)(n.code,{children:"curl 10.4.2.45"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"But now let's try the echo service itself:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ curl echo.echo\ncluster1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["That works! How strange! So istio is ",(0,r.jsx)(n.em,{children:"capable"})," of routing our traffic over mTLS,\nit just isn't when connecting to the ",(0,r.jsx)(n.strong,{children:"pod IP"})," rather than service."]}),"\n",(0,r.jsx)(n.p,{children:"Okay, so what about service IP? Let's get the ip of the associated echo service:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ kubectl get svc -n echo\nNAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\necho            ClusterIP   10.5.113.88   <none>        80/TCP    48m\n"})}),"\n",(0,r.jsx)(n.p,{children:"And now let's curl that:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ curl 10.5.113.88\ncluster1\n"})}),"\n",(0,r.jsxs)(n.p,{children:["That also works! So from this we can conclude that Istio is routing things correctly\nwhen using the service name or service IP, but not when using the pod IP. Upon further\nreflection on the Istio documentation on ",(0,r.jsx)(n.a,{href:"https://istio.io/latest/docs/reference/config/networking/workload-entry/",children:"WorkloadEntry"}),",\nthese lines stand out:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"...\n  # use of the service account indicates that the workload has a\n  # sidecar proxy bootstrapped with this service account. Pods with\n  # sidecars will automatically communicate with the workload using\n  # istio mutual TLS.\n  serviceAccount: details-legacy\n"})}),"\n",(0,r.jsx)(n.p,{children:"This means that when working with a WorkloadEntry, Istio looks for a serviceAccount\nassociated with the pod to communicate over mTLS. So let's add that:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: networking.istio.io/v1\nkind: WorkloadEntry\nmetadata:\n  labels:\n    global-service: global-echo\n  name: global-echo-cluster1-10.4.2.45\n  namespace: curl\nspec:\n  address: 10.4.2.45\n  labels:\n    global-service: global-echo\n  serviceAccount: client # <-----   Added this\n---\napiVersion: networking.istio.io/v1\nkind: WorkloadEntry\nmetadata:\n  labels:\n    global-service: global-echo\n  name: global-echo-cluster2-10.6.1.212\n  namespace: curl\nspec:\n  address: 10.6.1.212\n  labels:\n    global-service: global-echo\n  serviceAccount: client # <-----   Added this\n"})}),"\n",(0,r.jsx)(n.p,{children:"Now, let's try our global-echo curl:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ curl global-echo\nupstream connect error or disconnect/reset before headers. retried and the latest reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Weird. Now we're getting a certificate issue. I suppose this means Istio is ",(0,r.jsx)(n.em,{children:"trying"}),"\nto communicate with the pod over mTLS, but there's clearly a certificate issue going wrong.\nIt's time to look at the Istio logs."]}),"\n",(0,r.jsxs)(n.p,{children:["On a successful request (",(0,r.jsx)(n.code,{children:"curl 10.5.113.88"}),"), we see Istio logs coming from the client\nsidecar:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'2025-04-29T17:44:56.975202Z\tdebug\tenvoy filter external/envoy/source/extensions/filters/listener/original_dst/original_dst.cc:69\toriginal_dst: set destination to 10.5.113.88:80\tthread=28\n2025-04-29T17:44:56.975404Z\tdebug\tenvoy filter external/envoy/source/extensions/filters/listener/http_inspector/http_inspector.cc:139\thttp inspector: set application protocol to http/1.1\tthread=28\n2025-04-29T17:44:56.975558Z\tdebug\tenvoy conn_handler external/envoy/source/common/listener_manager/active_tcp_listener.cc:160\t[Tags: "ConnectionId":"192"] new connection from 10.4.2.55:49952\tthread=28\n2025-04-29T17:44:56.975643Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:393\t[Tags: "ConnectionId":"192"] new stream\tthread=28\n2025-04-29T17:44:56.975821Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1183\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] request headers complete (end_stream=true):\n\':authority\', \'10.5.113.88\'\n\':path\', \'/\'\n\':method\', \'GET\'\n\'user-agent\', \'curl/8.7.1\'\n\'accept\', \'*/*\'\n\tthread=28\n2025-04-29T17:44:56.975840Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1166\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] request end stream timestamp recorded\tthread=28\n2025-04-29T17:44:56.975870Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\t[Tags: "ConnectionId":"192"] current connecting state: false\tthread=28\n2025-04-29T17:44:56.975922Z\tdebug\tenvoy filter source/extensions/filters/http/alpn/alpn_filter.cc:92\toverride with 3 ALPNs\tthread=28\n2025-04-29T17:44:56.975938Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:527\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] cluster \'outbound|80||echo.echo.svc.cluster.local\' match for URL \'/\'\tthread=28\n2025-04-29T17:44:56.975979Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:756\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] router decoding headers:\n\':authority\', \'10.5.113.88\'\n\':path\', \'/\'\n\':method\', \'GET\'\n\':scheme\', \'http\'\n\'user-agent\', \'curl/8.7.1\'\n\'accept\', \'*/*\'\n\'x-forwarded-proto\', \'http\'\n\'x-request-id\', \'79f4055e-4122-4096-af82-6dcbebeb8498\'\n\'x-envoy-decorator-operation\', \'echo.echo.svc.cluster.local:80/*\'\n\'x-envoy-peer-metadata-id\', \'sidecar~10.4.2.55~client-f4cd469d6-wnsrx.curl~curl.svc.cluster.local\'\n\'x-envoy-peer-metadata\', \'ChoKCkNMVVNURVJfSUQSDBoKS3ViZXJuZXRlcwqIAQoGTEFCRUxTEn4qfAoPCgNhcHASCBoGY2xpZW50CisKH3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLW5hbWUSCBoGY2xpZW50CisKI3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLXJldmlzaW9uEgQaAnYxCg8KB3ZlcnNpb24SBBoCdjEKIAoETkFNRRIYGhZjbGllbnQtZjRjZDQ2OWQ2LXduc3J4ChMKCU5BTUVTUEFDRRIGGgRjdXJsCkcKBU9XTkVSEj4aPGt1YmVybmV0ZXM6Ly9hcGlzL2FwcHMvdjEvbmFtZXNwYWNlcy9jdXJsL2RlcGxveW1lbnRzL2NsaWVudAoZCg1XT1JLTE9BRF9OQU1FEggaBmNsaWVudA==\'\n\'x-envoy-attempt-count\', \'1\'\n\tthread=28\n2025-04-29T17:44:56.976066Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:265\t[Tags: "ConnectionId":"181"] using existing fully connected connection\tthread=28\n2025-04-29T17:44:56.976071Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:182\t[Tags: "ConnectionId":"181"] creating stream\tthread=28\n2025-04-29T17:44:56.976084Z\tdebug\tenvoy router external/envoy/source/common/router/upstream_request.cc:593\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] pool ready\tthread=28\n2025-04-29T17:44:56.976117Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:142\t[Tags: "ConnectionId":"181"] encode complete\tthread=28\n2025-04-29T17:44:56.977190Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:1559\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] upstream headers complete: end_stream=false\tthread=28\n2025-04-29T17:44:56.977274Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1878\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] encoding headers via codec (end_stream=false):\n\':status\', \'200\'\n\'x-app-name\', \'http-echo\'\n\'x-app-version\', \'1.0.0\'\n\'date\', \'Tue, 29 Apr 2025 17:44:56 GMT\'\n\'content-length\', \'9\'\n\'content-type\', \'text/plain; charset=utf-8\'\n\'x-envoy-upstream-service-time\', \'1\'\n\'server\', \'envoy\'\n\tthread=28\n2025-04-29T17:44:56.977301Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:129\t[Tags: "ConnectionId":"181"] response complete\tthread=28\n2025-04-29T17:44:56.977316Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1993\t[Tags: "ConnectionId":"192","StreamId":"12612765843006107830"] Codec completed encoding stream.\tthread=28\n2025-04-29T17:44:56.977361Z\tdebug\tenvoy pool external/envoy/source/common/http/http1/conn_pool.cc:53\t[Tags: "ConnectionId":"181"] response complete\tthread=28\n2025-04-29T17:44:56.977368Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:215\t[Tags: "ConnectionId":"181"] destroying stream: 0 remaining\tthread=28\n2025-04-29T17:44:56.977832Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:714\t[Tags: "ConnectionId":"192"] remote close\tthread=28\n2025-04-29T17:44:56.977856Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\t[Tags: "ConnectionId":"192"] closing socket: 0\tthread=28\n2025-04-29T17:44:56.977944Z\tdebug\tenvoy conn_handler external/envoy/source/common/listener_manager/active_stream_listener_base.cc:136\t[Tags: "ConnectionId":"192"] adding to cleanup list\tthread=28\n'})}),"\n",(0,r.jsxs)(n.p,{children:["Note particularly the line ",(0,r.jsx)(n.code,{children:"cluster 'outbound|80||echo.echo.svc.cluster.local' match for URL '/'\tthread=28"}),".\nThis gives us some important information. Even on an IP route to the service,\nIstio is matching the request to the outbound service it has configured."]}),"\n",(0,r.jsxs)(n.p,{children:["Let's compare that to the log when we send a request to the ",(0,r.jsx)(n.code,{children:"global-echo"})," service:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'2025-04-29T18:08:46.944432Z\tdebug\tenvoy filter external/envoy/source/extensions/filters/listener/original_dst/original_dst.cc:69\toriginal_dst: set destination to 240.240.0.1:80\tthread=29\n2025-04-29T18:08:46.944666Z\tdebug\tenvoy filter external/envoy/source/extensions/filters/listener/http_inspector/http_inspector.cc:139\thttp inspector: set application protocol to http/1.1\tthread=29\n2025-04-29T18:08:46.944853Z\tdebug\tenvoy conn_handler external/envoy/source/common/listener_manager/active_tcp_listener.cc:160\t[Tags: "ConnectionId":"446"] new connection from 10.4.2.55:35226\tthread=29\n2025-04-29T18:08:46.944900Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:393\t[Tags: "ConnectionId":"446"] new stream\tthread=29\n2025-04-29T18:08:46.944966Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1183\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] request headers complete (end_stream=true):\n\':authority\', \'global-echo\'\n\':path\', \'/\'\n\':method\', \'GET\'\n\'user-agent\', \'curl/8.7.1\'\n\'accept\', \'*/*\'\n\tthread=29\n2025-04-29T18:08:46.944980Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1166\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] request end stream timestamp recorded\tthread=29\n2025-04-29T18:08:46.945014Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\t[Tags: "ConnectionId":"446"] current connecting state: false\tthread=29\n2025-04-29T18:08:46.945174Z\tdebug\tenvoy filter source/extensions/filters/http/alpn/alpn_filter.cc:92\toverride with 3 ALPNs\tthread=29\n2025-04-29T18:08:46.945206Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:527\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] cluster \'outbound|80||global-echo\' match for URL \'/\'\tthread=29\n2025-04-29T18:08:46.945308Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:756\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] router decoding headers:\n\':authority\', \'global-echo\'\n\':path\', \'/\'\n\':method\', \'GET\'\n\':scheme\', \'http\'\n\'user-agent\', \'curl/8.7.1\'\n\'accept\', \'*/*\'\n\'x-forwarded-proto\', \'http\'\n\'x-request-id\', \'c7b51081-47e1-491b-ba1e-a87bd4608220\'\n\'x-envoy-decorator-operation\', \'global-echo:80/*\'\n\'x-envoy-peer-metadata-id\', \'sidecar~10.4.2.55~client-f4cd469d6-wnsrx.curl~curl.svc.cluster.local\'\n\'x-envoy-peer-metadata\', \'ChoKCkNMVVNURVJfSUQSDBoKS3ViZXJuZXRlcwqIAQoGTEFCRUxTEn4qfAoPCgNhcHASCBoGY2xpZW50CisKH3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLW5hbWUSCBoGY2xpZW50CisKI3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLXJldmlzaW9uEgQaAnYxCg8KB3ZlcnNpb24SBBoCdjEKIAoETkFNRRIYGhZjbGllbnQtZjRjZDQ2OWQ2LXduc3J4ChMKCU5BTUVTUEFDRRIGGgRjdXJsCkcKBU9XTkVSEj4aPGt1YmVybmV0ZXM6Ly9hcGlzL2FwcHMvdjEvbmFtZXNwYWNlcy9jdXJsL2RlcGxveW1lbnRzL2NsaWVudAoZCg1XT1JLTE9BRF9OQU1FEggaBmNsaWVudA==\'\n\'x-envoy-attempt-count\', \'1\'\n\tthread=29\n2025-04-29T18:08:46.945334Z\tdebug\tenvoy pool external/envoy/source/common/http/conn_pool_base.cc:78\tqueueing stream due to no available connections (ready=0 busy=0 connecting=0)\tthread=29\n2025-04-29T18:08:46.945342Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:291\ttrying to create new connection\tthread=29\n2025-04-29T18:08:46.945346Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:145\tcreating a new connection (connecting=0)\tthread=29\n2025-04-29T18:08:46.945472Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\t[Tags: "ConnectionId":"447"] current connecting state: true\tthread=29\n2025-04-29T18:08:46.945480Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:57\t[Tags: "ConnectionId":"447"] connecting\tthread=29\n2025-04-29T18:08:46.945488Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1017\t[Tags: "ConnectionId":"447"] connecting to 10.4.2.45:8080\tthread=29\n2025-04-29T18:08:46.945663Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1036\t[Tags: "ConnectionId":"447"] connection in progress\tthread=29\n2025-04-29T18:08:46.945707Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:746\t[Tags: "ConnectionId":"447"] connected\tthread=29\n2025-04-29T18:08:46.947216Z\tdebug\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\tverify cert failed: SAN matcher\tthread=29\n2025-04-29T18:08:46.947284Z\tdebug\tenvoy connection external/envoy/source/common/tls/ssl_socket.cc:246\t[Tags: "ConnectionId":"447"] remote address:10.4.2.45:8080,TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:46.947297Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\t[Tags: "ConnectionId":"447"] closing socket: 0\tthread=29\n2025-04-29T18:08:46.947340Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:107\t[Tags: "ConnectionId":"447"] disconnect. resetting 0 pending requests\tthread=29\n2025-04-29T18:08:46.947378Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:495\t[Tags: "ConnectionId":"447"] client disconnected, failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:46.947403Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:1384\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] upstream reset: reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:46.947452Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:463\tinvoking 1 idle callback(s) - is_draining_for_deletion_=false\tthread=29\n2025-04-29T18:08:46.962109Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:2013\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] performing retry\tthread=29\n2025-04-29T18:08:46.962200Z\tdebug\tenvoy pool external/envoy/source/common/http/conn_pool_base.cc:78\tqueueing stream due to no available connections (ready=0 busy=0 connecting=0)\tthread=29\n2025-04-29T18:08:46.962208Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:291\ttrying to create new connection\tthread=29\n2025-04-29T18:08:46.962210Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:145\tcreating a new connection (connecting=0)\tthread=29\n2025-04-29T18:08:46.962340Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\t[Tags: "ConnectionId":"448"] current connecting state: true\tthread=29\n2025-04-29T18:08:46.962504Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:57\t[Tags: "ConnectionId":"448"] connecting\tthread=29\n2025-04-29T18:08:46.962520Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1017\t[Tags: "ConnectionId":"448"] connecting to 10.4.2.45:8080\tthread=29\n2025-04-29T18:08:46.962810Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1036\t[Tags: "ConnectionId":"448"] connection in progress\tthread=29\n2025-04-29T18:08:46.962854Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:746\t[Tags: "ConnectionId":"448"] connected\tthread=29\n2025-04-29T18:08:46.964802Z\tdebug\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\tverify cert failed: SAN matcher\tthread=29\n2025-04-29T18:08:46.964857Z\tdebug\tenvoy connection external/envoy/source/common/tls/ssl_socket.cc:246\t[Tags: "ConnectionId":"448"] remote address:10.4.2.45:8080,TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:46.964861Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\t[Tags: "ConnectionId":"448"] closing socket: 0\tthread=29\n2025-04-29T18:08:46.965002Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:107\t[Tags: "ConnectionId":"448"] disconnect. resetting 0 pending requests\tthread=29\n2025-04-29T18:08:46.965017Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:495\t[Tags: "ConnectionId":"448"] client disconnected, failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:46.965098Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:1384\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] upstream reset: reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:46.965141Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:463\tinvoking 1 idle callback(s) - is_draining_for_deletion_=false\tthread=29\n2025-04-29T18:08:47.013428Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:2013\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] performing retry\tthread=29\n2025-04-29T18:08:47.013521Z\tdebug\tenvoy pool external/envoy/source/common/http/conn_pool_base.cc:78\tqueueing stream due to no available connections (ready=0 busy=0 connecting=0)\tthread=29\n2025-04-29T18:08:47.013525Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:291\ttrying to create new connection\tthread=29\n2025-04-29T18:08:47.013547Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:145\tcreating a new connection (connecting=0)\tthread=29\n2025-04-29T18:08:47.013729Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\t[Tags: "ConnectionId":"449"] current connecting state: true\tthread=29\n2025-04-29T18:08:47.013755Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:57\t[Tags: "ConnectionId":"449"] connecting\tthread=29\n2025-04-29T18:08:47.013761Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1017\t[Tags: "ConnectionId":"449"] connecting to 10.4.2.45:8080\tthread=29\n2025-04-29T18:08:47.014191Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1036\t[Tags: "ConnectionId":"449"] connection in progress\tthread=29\n2025-04-29T18:08:47.014216Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:746\t[Tags: "ConnectionId":"449"] connected\tthread=29\n2025-04-29T18:08:47.015911Z\tdebug\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\tverify cert failed: SAN matcher\tthread=29\n2025-04-29T18:08:47.016058Z\tdebug\tenvoy connection external/envoy/source/common/tls/ssl_socket.cc:246\t[Tags: "ConnectionId":"449"] remote address:10.4.2.45:8080,TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:47.016062Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\t[Tags: "ConnectionId":"449"] closing socket: 0\tthread=29\n2025-04-29T18:08:47.016113Z\tdebug\tenvoy client external/envoy/source/common/http/codec_client.cc:107\t[Tags: "ConnectionId":"449"] disconnect. resetting 0 pending requests\tthread=29\n2025-04-29T18:08:47.016138Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:495\t[Tags: "ConnectionId":"449"] client disconnected, failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:47.016155Z\tdebug\tenvoy router external/envoy/source/common/router/router.cc:1384\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] upstream reset: reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\tthread=29\n2025-04-29T18:08:47.016271Z\tdebug\tenvoy http external/envoy/source/common/http/filter_manager.cc:1084\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] Sending local reply with details upstream_reset_before_response_started{remote_connection_failure|TLS_error:|268435581:SSL_routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end}\tthread=29\n2025-04-29T18:08:47.016338Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1878\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] encoding headers via codec (end_stream=false):\n\':status\', \'503\'\n\'content-length\', \'239\'\n\'content-type\', \'text/plain\'\n\'date\', \'Tue, 29 Apr 2025 18:08:46 GMT\'\n\'server\', \'envoy\'\n\tthread=29\n2025-04-29T18:08:47.016377Z\tdebug\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1993\t[Tags: "ConnectionId":"446","StreamId":"5689876678243589196"] Codec completed encoding stream.\tthread=29\n2025-04-29T18:08:47.016499Z\tdebug\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:463\tinvoking 1 idle callback(s) - is_draining_for_deletion_=false\tthread=29\n2025-04-29T18:08:47.017236Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:714\t[Tags: "ConnectionId":"446"] remote close\tthread=29\n2025-04-29T18:08:47.017259Z\tdebug\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\t[Tags: "ConnectionId":"446"] closing socket: 0\tthread=29\n2025-04-29T18:08:47.017318Z\tdebug\tenvoy conn_handler external/envoy/source/common/listener_manager/active_stream_listener_base.cc:136\t[Tags: "ConnectionId":"446"] adding to cleanup list\tthread=29\n'})}),"\n",(0,r.jsxs)(n.p,{children:["As you can see, the request is being handled by the ",(0,r.jsx)(n.code,{children:"outbound|80||global-echo"})," cluster,\nand yet the upstream server is still responding with a 503. There is a pertinent\npart here that hints at what might be going wrong:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"2025-04-29T18:08:46.947216Z\tdebug\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\tverify cert failed: SAN matcher\tthread=29\n"})}),"\n",(0,r.jsx)(n.p,{children:"our SAN isn't getting matched. For anyone not super familiar, a\nSAN stands for Subject Alternative Name. It is an extension in X.509 certificates (used in TLS/SSL) that allows you to\nspecify additional identities (such as DNS names, IP addresses, or URIs) that the certificate should be valid for,\nbeyond the primary Common Name (CN). Istio uses this to verify the spiffeID\nassociated with the serviceAccount used in the mTLS handshake."}),"\n",(0,r.jsx)(n.p,{children:"Let's debug the cluster configuration next in envoy and see if there's a difference."}),"\n",(0,r.jsxs)(n.p,{children:["(Commands gotten with ",(0,r.jsx)(n.code,{children:"istioctl pc cluster -n curl curl-f4cd469d6-wnsrx --fqdn echo.echo.svc.cluster.local -o j son"}),")"]}),"\n",(0,r.jsxs)(n.p,{children:["Diff between cluster configs of not working (",(0,r.jsx)(n.code,{children:"-"}),") and not working (",(0,r.jsx)(n.code,{children:"+"}),")"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-diff",children:'                  "defaultValidationContext": {\n                    "matchSubjectAltNames": [\n                      {\n-                       "exact": "spiffe://cluster.local/ns/curl/sa/curl"\n                      }\n                    ]\n                  },\n---\n                  "defaultValidationContext": {\n                    "matchSubjectAltNames": [\n                      {\n+                       "exact": "spiffe://cluster.local/ns/echo/sa/default"\n                      }\n                    ]\n                  },\n'})}),"\n",(0,r.jsx)(n.p,{children:"(Note the output has been cleaned to show the pertinent, non-trivial parts)"}),"\n",(0,r.jsxs)(n.p,{children:["This is the key. Inside of Istio's tls configuration, the configured SAN in\nthe working configuration is associated with the ",(0,r.jsx)(n.em,{children:"destination"})," service account,\nwhile in the non-working configuration, it is associated with the ",(0,r.jsx)(n.em,{children:"source"})," service account."]}),"\n",(0,r.jsx)(n.p,{children:"According to the Istio documentation, we can edit the SAN inside of our ServiceEntry:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"apiVersion: networking.istio.io/v1\nkind: ServiceEntry\nmetadata:\n  name: global-curl-global-echo\n  namespace: curl\nspec:\n  hosts:\n    - global-echo\n  location: MESH_INTERNAL\n  ports:\n    - name: http\n      number: 80\n      protocol: HTTP\n      targetPort: 8080\n  resolution: STATIC\n  subjectAltNames:\n    - spiffe://cluster.local/ns/echo/sa/default # <----- Added this\n  workloadSelector:\n    labels:\n      global-service: global-echo\n"})}),"\n",(0,r.jsx)(n.p,{children:"And once that's applied, we can test our custom host:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"$ curl global-echo\ncluster1\n"})}),"\n",(0,r.jsx)(n.p,{children:"It works! So now we have a custom host routing directly between pod IPs."}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"In this post, we learned how to set up a custom host in Istio that routes between\ntwo different clusters. We learned how to set up a ServiceEntry and WorkloadEntry\nto route between the two clusters, and how to set up a custom SAN in the ServiceEntry\nto allow for mTLS communication between the two clusters. We also learned how to\ndebug the Istio configuration to find out what was going wrong with our mTLS\nconfiguration, and how to fix it by adding the correct SAN to the ServiceEntry."}),"\n",(0,r.jsx)(n.h3,{id:"related-issues",children:"Related Issues"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/istio/istio/issues/37431",children:"https://github.com/istio/istio/issues/37431"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://discuss.istio.io/t/503-between-pod-to-pod-communication-1-5-1/6121",children:"https://discuss.istio.io/t/503-between-pod-to-pod-communication-1-5-1/6121"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://stackoverflow.com/questions/62881298/what-is-pod-to-pod-encryption-in-kubernetes-and-how-to-implement-pod-to-pod-enc",children:"https://stackoverflow.com/questions/62881298/what-is-pod-to-pod-encryption-in-kubernetes-and-how-to-implement-pod-to-pod-enc"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,c.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(6540);const r={},c=o.createContext(r);function s(e){const n=o.useContext(c);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(c.Provider,{value:n},e.children)}}}]);