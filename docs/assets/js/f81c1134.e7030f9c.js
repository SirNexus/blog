"use strict";(self.webpackChunkaidan_blog=self.webpackChunkaidan_blog||[]).push([[130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"kind-multi-cluster-flat-network","metadata":{"permalink":"/blog/kind-multi-cluster-flat-network","source":"@site/blog/2025-06-13-kind-multi-cluster-flat-network.md","title":"Networking Multiple Kind Kubernetes Clusters Together Using Native Routing","description":"Recently, I set out to create a multi-cluster kind environment where clusters could communicate over a flat network.","date":"2025-06-13T00:00:00.000Z","tags":[{"inline":true,"label":"Kind","permalink":"/blog/tags/kind"},{"inline":true,"label":"Kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"Networking","permalink":"/blog/tags/networking"},{"inline":true,"label":"Pod","permalink":"/blog/tags/pod"},{"inline":true,"label":"Linux","permalink":"/blog/tags/linux"}],"readingTime":2.87,"hasTruncateMarker":true,"authors":[{"name":"Aidan Carson","key":"aidancarson","page":null}],"frontMatter":{"slug":"kind-multi-cluster-flat-network","title":"Networking Multiple Kind Kubernetes Clusters Together Using Native Routing","authors":"aidancarson","tags":["Kind","Kubernetes","Networking","Pod","Linux"],"toc_min_heading_level":2,"toc_max_heading_level":5},"unlisted":false,"nextItem":{"title":"Enabling pod to pod mTLS in Istio","permalink":"/blog/istio-pod-to-pod-mtls"}},"content":"Recently, I set out to create a multi-cluster kind environment where clusters could communicate over a flat network. \\nThe goal was simple: pods in one cluster should be able to directly talk to pods in another cluster without requiring tunneling or proxies.\\n\\nAt first glance, this seemed tricky \u2014 the pod IPs assigned inside kind clusters exist only within the containerized \\nnetwork of each cluster, and Docker isolates these networks by default. However, I eventually found a straightforward \\nsolution that uses native routing, and I wanted to share my journey and what worked.\\n\\n## The Challenge\\nBy design, kind runs Kubernetes clusters inside Docker containers. This means:\\n\\n* Pod IPs are only visible inside the Docker network created for each kind cluster.\\n* There\u2019s no built-in way for a pod in one cluster to directly communicate with a pod in another cluster using its pod IP.\\n* Bridging these isolated networks requires either complex overlay solutions or manual routing.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Options Considered\\n1. Use Cilium Service Mesh\\n   * If you\u2019re okay with the added configuration and overhead, Cilium Service Mesh is a great choice:\\n   * It can establish a flat network across multiple clusters.\\n   * With [Global Service Affinity](https://docs.cilium.io/en/stable/network/clustermesh/affinity/#enabling-global-service-affinity),\\n     services in different clusters can even resolve each other\u2019s names and load balance between them.\\n\\n\\n2. Native Routing Using ip route (The solution that worked): manually add routes \\n   to each Docker container that runs a kind node.\\n   * Each node knows how to route pod traffic destined for another cluster\u2019s pod subnet.\\n   * Traffic flows directly via the Docker bridge network, using the container IP of the appropriate node as the next hop.\\n\\n\\nFor my use case, I wanted a lighter-weight solution that didn\u2019t require\\ndeploying cilium cluster mesh, so I opted for the native routing approach.\\n\\n## Example Configuration\\n\\nHere\u2019s an example kind config I used to define pod and service subnets:\\n\\n```yaml\\n\\nkind: Cluster\\napiVersion: kind.x-k8s.io/v1alpha4\\nnodes:\\n   - role: control-plane\\n   - role: worker\\n     networking:\\n     disableDefaultCNI: true\\n     podSubnet: \\"10.0.0.0/16\\"\\n     serviceSubnet: \\"10.1.0.0/16\\"\\n```\\nTo make routing work, I added routes like:\\n\\n```bash\\nip route add 10.0.0.0/16 via <docker-container-ip-of-cluster1-node>\\nip route add 10.2.0.0/16 via <docker-container-ip-of-cluster2-node>\\n```\\nEach via IP corresponds to a Docker container (the kind node) that knows how to reach its local pod subnet.\\n\\n## Example Workflow\\n1. Create two kind clusters with distinct pod subnets:\\n\\nCluster A: 10.0.0.0/16\\n\\nCluster B: 10.2.0.0/16\\n\\n2. Retrieve the Docker container IPs for the nodes in each cluster:\\n\\n```bash\\ndocker inspect -f \'{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}\' <container-name>\\n```\\n3. On each kind node container, add routes for the other cluster\u2019s pod subnet:\\n\\n```bash\\ndocker exec <cluster-a-node> ip route add 10.2.0.0/16 via <cluster-b-node-ip>\\ndocker exec <cluster-b-node> ip route add 10.0.0.0/16 via <cluster-a-node-ip>\\n```\\n\\nThe Result: Pods in Cluster A can now talk to pods in Cluster B by pod IP, and vice versa!\\n\\n## Final Thoughts\\nIf you\u2019re experimenting with multi-cluster setups in a local or CI environment, \\nthis native routing method is lightweight and effective. That said, for production-like features (service discovery, identity-aware routing, encryption), service mesh solutions like Cilium offer more power and flexibility.\\n\\n## A note for Cilium and service networking\\n\\nIf you\'re using Cilium for your CNI, you\'ll want to use the `bpf.lbExternalClusterIP`\\noption in helm. This will expose the cluster IPs to the host network on the nodes,\\nallowing you to also address the service IPs from your other clusters.\\n\\nHappy clustering! If you have any questions or suggestions, feel free to reach out."},{"id":"istio-pod-to-pod-mtls","metadata":{"permalink":"/blog/istio-pod-to-pod-mtls","source":"@site/blog/2025-04-29-istio-pod-to-pod-mtls.md","title":"Enabling pod to pod mTLS in Istio","description":"Imagine a situation where you have multiple different clusters, each running","date":"2025-04-29T00:00:00.000Z","tags":[{"inline":true,"label":"Istio","permalink":"/blog/tags/istio"},{"inline":true,"label":"ServiceEntry","permalink":"/blog/tags/service-entry"},{"inline":true,"label":"pod","permalink":"/blog/tags/pod"},{"inline":true,"label":"mTLS","permalink":"/blog/tags/m-tls"},{"inline":true,"label":"strict","permalink":"/blog/tags/strict"},{"inline":true,"label":"authentication","permalink":"/blog/tags/authentication"}],"readingTime":13.73,"hasTruncateMarker":true,"authors":[{"name":"Aidan Carson","key":"aidancarson","page":null}],"frontMatter":{"slug":"istio-pod-to-pod-mtls","title":"Enabling pod to pod mTLS in Istio","authors":"aidancarson","tags":["Istio","ServiceEntry","pod","mTLS","strict","authentication"],"toc_min_heading_level":2,"toc_max_heading_level":5},"unlisted":false,"prevItem":{"title":"Networking Multiple Kind Kubernetes Clusters Together Using Native Routing","permalink":"/blog/kind-multi-cluster-flat-network"}},"content":"Imagine a situation where you have multiple different clusters, each running\\nan Istio service mesh and which are federated together to talk to each other.\\nNow also imagine that these clusters are networked together such that each\\npod IP is uniquely addressable and able to be communicated with from any other\\ncluster.\\n\\n## Context\\n\\nI faced a situation like this, where I needed to group endpoints into logical\\nhostnames that represented services backed by those endpoints. Because endpoints\\ncould live anywhere, in any cluster, I landed on using a ServiceEntry to register\\nthe hostname and WorkloadEntries to represent the endpoints service that hostname.\\n\\nBut a problem came on enabling PeerAuthentication:\\n\\n```yaml\\napiVersion: security.istio.io/v1\\nkind: PeerAuthentication\\nmetadata:\\n  name: default\\n  namespace: prod-istio-system # The namespace of our istio installation\\nspec:\\n  mtls:\\n    mode: STRICT\\n```\\n\\nCurl stopped working!\\n\\n```bash\\n$ curl global-echo\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\n```\\n\\nThrough some debugging, I was able to get it to:\\n```bash\\n$ curl global-echo\\nupstream connect error or disconnect/reset before headers. retried and the latest reset reason: remote connection failure, \\ntransport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\n```\\n\\nBut to find the true root cause and solution took some digging.\\n\\nLet\'s get into it\\n\\n\x3c!-- truncate --\x3e\\n\\n## Setup\\n\\nThe configuration looked something like this:\\n\\nServiceEntry:\\n```yaml\\napiVersion: v1\\nitems:\\n  - apiVersion: networking.istio.io/v1\\n    kind: ServiceEntry\\n    metadata:\\n      creationTimestamp: \\"2025-04-29T17:01:04Z\\"\\n      generation: 7\\n      name: global-curl-global-echo\\n      namespace: curl\\n      resourceVersion: \\"7068\\"\\n      uid: 9c26a814-e852-4903-81a9-9213987236a0\\n    spec:\\n      hosts:\\n        - global-echo\\n      location: MESH_INTERNAL\\n      ports:\\n        - name: http\\n          number: 80\\n          protocol: HTTP\\n          targetPort: 8080\\n      resolution: STATIC\\n      subjectAltNames:\\n        - spiffe://cluster.local/ns/echo/sa/default\\n        - spiffe://cluster.local/ns/echo/sa/default\\n      workloadSelector:\\n        labels:\\n          global-service: global-echo\\n    status:\\n      addresses:\\n        - host: global-echo\\n          value: 240.240.0.1\\n        - host: global-echo\\n          value: 2001:2::1\\nkind: List\\nmetadata:\\n  resourceVersion: \\"\\"\\n```\\nWorkloadEntry:\\n```\\nNAME                              AGE     ADDRESS\\nglobal-echo-cluster1-10.4.2.45    4m58s   10.4.2.45\\nglobal-echo-cluster2-10.6.1.212   27m     10.6.1.212\\n```\\n\\n\\n```yaml\\napiVersion: v1\\nitems:\\n  - apiVersion: networking.istio.io/v1\\n    kind: WorkloadEntry\\n    metadata:\\n      creationTimestamp: \\"2025-04-29T17:24:24Z\\"\\n      generation: 2\\n      labels:\\n        global-service: global-echo\\n      name: global-echo-cluster1-10.4.2.45\\n      namespace: curl\\n      resourceVersion: \\"7188\\"\\n      uid: d8c0a5da-aab8-4338-b890-b597a7883f47\\n    spec:\\n      address: 10.4.2.45\\n      labels:\\n        global-service: global-echo\\n  - apiVersion: networking.istio.io/v1\\n    kind: WorkloadEntry\\n    metadata:\\n      creationTimestamp: \\"2025-04-29T17:01:43Z\\"\\n      generation: 4\\n      labels:\\n        global-service: global-echo\\n      name: global-echo-cluster2-10.6.1.212\\n      namespace: curl\\n      resourceVersion: \\"7189\\"\\n      uid: 72ff1595-e098-4eb2-9f84-f251d3c8faf4\\n    spec:\\n      address: 10.6.1.212\\n      labels:\\n        global-service: global-echo\\nkind: List\\nmetadata:\\n  resourceVersion: \\"\\"\\n\\n```\\n\\nThese resources specify a host `global-echo`, and two backend services each of which\\nlive in their own clusters. Behind the scenes, I have these services running simple\\necho servers. If you\'d like that yaml, here it is:\\n\\n```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: echo\\n  namespace: echo\\nspec:\\n  replicas: 1\\n  selector:\\n    matchLabels:\\n      app: echo\\n  template:\\n    metadata:\\n      labels:\\n        app: echo\\n    spec:\\n      containers:\\n        - name: echo\\n          image: hashicorp/http-echo\\n          args:\\n            - -text={{ .CurCluster }}\\n            - -listen=:8080\\n          resources:\\n            requests:\\n              cpu: \\"100m\\"\\n              memory: \\"100Mi\\"\\n            limits:\\n              cpu: \\"100m\\"\\n              memory: \\"100Mi\\"\\n```\\n\\nAnd it works! I can curl both pods in my mesh:\\n\\n```\\nfor i in {1..10}; do curl global-echo; done\\ncluster1\\ncluster1\\ncluster1\\ncluster1\\ncluster2\\ncluster2\\ncluster2\\ncluster1\\ncluster1\\ncluster2\\n```\\n\\n## The Problem\\n\\nSo networking is working. Istio is generating the correct Envoy config such that\\nour global hostname routes to our backing services on each cluster. But what about\\nsecurity? Just to make sure that Istio is actually performing mTLS between pods,\\nlet\'s turn on strict mTLS.\\n\\n``` bash\\n$ kubectl apply -f - <<EOF\\napiVersion: security.istio.io/v1\\nkind: PeerAuthentication\\nmetadata:\\n  name: default\\n  namespace: prod-istio-system # The namespace of our istio installation\\nspec:\\n  mtls:\\n    mode: STRICT\\nEOF\\n```\\n\\nLet\'s check that our communication still works:\\n\\n```bash\\n$ for i in {1..10}; do curl global-echo; done\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\n```\\n\\nOur connection broke! What gives! I thought Istio was routing our services? Spoiler:\\nour connection broke because istio was using PERMISSIVE mode and sending our traffic\\nover plaintext. Now that we are enforcing STRICT mTLS mode, Istio is breaking.\\nThis means that we weren\'t secure before, sending traffic over plaintext. This is\\nsuper interesting. Istio should be handling the upgrading of our communication to\\nmTLS by default. So what\'s happening? Let\'s do some testing.\\n\\n## Finding the Solution\\n\\nSo let\'s look at the local endpoint. Curling from cluster1, we have a local echo\\npod IP of `10.4.2.45`. Let\'s curl that:\\n\\n```bash\\n$ curl 10.4.2.45\\nupstream connect error or disconnect/reset before headers. reset reason: connection termination\\n```\\n\\nHm, so that\'s broken. That makes sense, since the service entry is just resolving\\nDNS requests to the WorkloadEntry specifying `curl 10.4.2.45`.\\n\\nBut now let\'s try the echo service itself:\\n\\n```bash\\n$ curl echo.echo\\ncluster1\\n```\\n\\nThat works! How strange! So istio is *capable* of routing our traffic over mTLS,\\nit just isn\'t when connecting to the **pod IP** rather than service.\\n\\nOkay, so what about service IP? Let\'s get the ip of the associated echo service:\\n\\n```bash\\n$ kubectl get svc -n echo\\nNAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\\necho            ClusterIP   10.5.113.88   <none>        80/TCP    48m\\n```\\n\\nAnd now let\'s curl that:\\n\\n```bash\\n$ curl 10.5.113.88\\ncluster1\\n```\\n\\nThat also works! So from this we can conclude that Istio is routing things correctly\\nwhen using the service name or service IP, but not when using the pod IP. Upon further\\nreflection on the Istio documentation on [WorkloadEntry](https://istio.io/latest/docs/reference/config/networking/workload-entry/),\\nthese lines stand out:\\n\\n```\\n...\\n  # use of the service account indicates that the workload has a\\n  # sidecar proxy bootstrapped with this service account. Pods with\\n  # sidecars will automatically communicate with the workload using\\n  # istio mutual TLS.\\n  serviceAccount: details-legacy\\n```\\n\\nThis means that when working with a WorkloadEntry, Istio looks for a serviceAccount\\nassociated with the pod to communicate over mTLS. So let\'s add that:\\n\\n```yaml\\napiVersion: networking.istio.io/v1\\nkind: WorkloadEntry\\nmetadata:\\n  labels:\\n    global-service: global-echo\\n  name: global-echo-cluster1-10.4.2.45\\n  namespace: curl\\nspec:\\n  address: 10.4.2.45\\n  labels:\\n    global-service: global-echo\\n  serviceAccount: client # <-----   Added this\\n---\\napiVersion: networking.istio.io/v1\\nkind: WorkloadEntry\\nmetadata:\\n  labels:\\n    global-service: global-echo\\n  name: global-echo-cluster2-10.6.1.212\\n  namespace: curl\\nspec:\\n  address: 10.6.1.212\\n  labels:\\n    global-service: global-echo\\n  serviceAccount: client # <-----   Added this\\n```\\n\\nNow, let\'s try our global-echo curl:\\n\\n```bash\\n$ curl global-echo\\nupstream connect error or disconnect/reset before headers. retried and the latest reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\n```\\n\\nWeird. Now we\'re getting a certificate issue. I suppose this means Istio is *trying*\\nto communicate with the pod over mTLS, but there\'s clearly a certificate issue going wrong.\\nIt\'s time to look at the Istio logs.\\n\\nOn a successful request (`curl 10.5.113.88`), we see Istio logs coming from the client\\nsidecar:\\n```\\n2025-04-29T17:44:56.975202Z\\tdebug\\tenvoy filter external/envoy/source/extensions/filters/listener/original_dst/original_dst.cc:69\\toriginal_dst: set destination to 10.5.113.88:80\\tthread=28\\n2025-04-29T17:44:56.975404Z\\tdebug\\tenvoy filter external/envoy/source/extensions/filters/listener/http_inspector/http_inspector.cc:139\\thttp inspector: set application protocol to http/1.1\\tthread=28\\n2025-04-29T17:44:56.975558Z\\tdebug\\tenvoy conn_handler external/envoy/source/common/listener_manager/active_tcp_listener.cc:160\\t[Tags: \\"ConnectionId\\":\\"192\\"] new connection from 10.4.2.55:49952\\tthread=28\\n2025-04-29T17:44:56.975643Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:393\\t[Tags: \\"ConnectionId\\":\\"192\\"] new stream\\tthread=28\\n2025-04-29T17:44:56.975821Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1183\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] request headers complete (end_stream=true):\\n\':authority\', \'10.5.113.88\'\\n\':path\', \'/\'\\n\':method\', \'GET\'\\n\'user-agent\', \'curl/8.7.1\'\\n\'accept\', \'*/*\'\\n\\tthread=28\\n2025-04-29T17:44:56.975840Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1166\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] request end stream timestamp recorded\\tthread=28\\n2025-04-29T17:44:56.975870Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\\t[Tags: \\"ConnectionId\\":\\"192\\"] current connecting state: false\\tthread=28\\n2025-04-29T17:44:56.975922Z\\tdebug\\tenvoy filter source/extensions/filters/http/alpn/alpn_filter.cc:92\\toverride with 3 ALPNs\\tthread=28\\n2025-04-29T17:44:56.975938Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:527\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] cluster \'outbound|80||echo.echo.svc.cluster.local\' match for URL \'/\'\\tthread=28\\n2025-04-29T17:44:56.975979Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:756\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] router decoding headers:\\n\':authority\', \'10.5.113.88\'\\n\':path\', \'/\'\\n\':method\', \'GET\'\\n\':scheme\', \'http\'\\n\'user-agent\', \'curl/8.7.1\'\\n\'accept\', \'*/*\'\\n\'x-forwarded-proto\', \'http\'\\n\'x-request-id\', \'79f4055e-4122-4096-af82-6dcbebeb8498\'\\n\'x-envoy-decorator-operation\', \'echo.echo.svc.cluster.local:80/*\'\\n\'x-envoy-peer-metadata-id\', \'sidecar~10.4.2.55~client-f4cd469d6-wnsrx.curl~curl.svc.cluster.local\'\\n\'x-envoy-peer-metadata\', \'ChoKCkNMVVNURVJfSUQSDBoKS3ViZXJuZXRlcwqIAQoGTEFCRUxTEn4qfAoPCgNhcHASCBoGY2xpZW50CisKH3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLW5hbWUSCBoGY2xpZW50CisKI3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLXJldmlzaW9uEgQaAnYxCg8KB3ZlcnNpb24SBBoCdjEKIAoETkFNRRIYGhZjbGllbnQtZjRjZDQ2OWQ2LXduc3J4ChMKCU5BTUVTUEFDRRIGGgRjdXJsCkcKBU9XTkVSEj4aPGt1YmVybmV0ZXM6Ly9hcGlzL2FwcHMvdjEvbmFtZXNwYWNlcy9jdXJsL2RlcGxveW1lbnRzL2NsaWVudAoZCg1XT1JLTE9BRF9OQU1FEggaBmNsaWVudA==\'\\n\'x-envoy-attempt-count\', \'1\'\\n\\tthread=28\\n2025-04-29T17:44:56.976066Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:265\\t[Tags: \\"ConnectionId\\":\\"181\\"] using existing fully connected connection\\tthread=28\\n2025-04-29T17:44:56.976071Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:182\\t[Tags: \\"ConnectionId\\":\\"181\\"] creating stream\\tthread=28\\n2025-04-29T17:44:56.976084Z\\tdebug\\tenvoy router external/envoy/source/common/router/upstream_request.cc:593\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] pool ready\\tthread=28\\n2025-04-29T17:44:56.976117Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:142\\t[Tags: \\"ConnectionId\\":\\"181\\"] encode complete\\tthread=28\\n2025-04-29T17:44:56.977190Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:1559\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] upstream headers complete: end_stream=false\\tthread=28\\n2025-04-29T17:44:56.977274Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1878\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] encoding headers via codec (end_stream=false):\\n\':status\', \'200\'\\n\'x-app-name\', \'http-echo\'\\n\'x-app-version\', \'1.0.0\'\\n\'date\', \'Tue, 29 Apr 2025 17:44:56 GMT\'\\n\'content-length\', \'9\'\\n\'content-type\', \'text/plain; charset=utf-8\'\\n\'x-envoy-upstream-service-time\', \'1\'\\n\'server\', \'envoy\'\\n\\tthread=28\\n2025-04-29T17:44:56.977301Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:129\\t[Tags: \\"ConnectionId\\":\\"181\\"] response complete\\tthread=28\\n2025-04-29T17:44:56.977316Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1993\\t[Tags: \\"ConnectionId\\":\\"192\\",\\"StreamId\\":\\"12612765843006107830\\"] Codec completed encoding stream.\\tthread=28\\n2025-04-29T17:44:56.977361Z\\tdebug\\tenvoy pool external/envoy/source/common/http/http1/conn_pool.cc:53\\t[Tags: \\"ConnectionId\\":\\"181\\"] response complete\\tthread=28\\n2025-04-29T17:44:56.977368Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:215\\t[Tags: \\"ConnectionId\\":\\"181\\"] destroying stream: 0 remaining\\tthread=28\\n2025-04-29T17:44:56.977832Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:714\\t[Tags: \\"ConnectionId\\":\\"192\\"] remote close\\tthread=28\\n2025-04-29T17:44:56.977856Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\\t[Tags: \\"ConnectionId\\":\\"192\\"] closing socket: 0\\tthread=28\\n2025-04-29T17:44:56.977944Z\\tdebug\\tenvoy conn_handler external/envoy/source/common/listener_manager/active_stream_listener_base.cc:136\\t[Tags: \\"ConnectionId\\":\\"192\\"] adding to cleanup list\\tthread=28\\n```\\n\\nNote particularly the line `cluster \'outbound|80||echo.echo.svc.cluster.local\' match for URL \'/\'\\tthread=28`.\\nThis gives us some important information. Even on an IP route to the service,\\nIstio is matching the request to the outbound service it has configured.\\n\\nLet\'s compare that to the log when we send a request to the `global-echo` service:\\n\\n```\\n2025-04-29T18:08:46.944432Z\\tdebug\\tenvoy filter external/envoy/source/extensions/filters/listener/original_dst/original_dst.cc:69\\toriginal_dst: set destination to 240.240.0.1:80\\tthread=29\\n2025-04-29T18:08:46.944666Z\\tdebug\\tenvoy filter external/envoy/source/extensions/filters/listener/http_inspector/http_inspector.cc:139\\thttp inspector: set application protocol to http/1.1\\tthread=29\\n2025-04-29T18:08:46.944853Z\\tdebug\\tenvoy conn_handler external/envoy/source/common/listener_manager/active_tcp_listener.cc:160\\t[Tags: \\"ConnectionId\\":\\"446\\"] new connection from 10.4.2.55:35226\\tthread=29\\n2025-04-29T18:08:46.944900Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:393\\t[Tags: \\"ConnectionId\\":\\"446\\"] new stream\\tthread=29\\n2025-04-29T18:08:46.944966Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1183\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] request headers complete (end_stream=true):\\n\':authority\', \'global-echo\'\\n\':path\', \'/\'\\n\':method\', \'GET\'\\n\'user-agent\', \'curl/8.7.1\'\\n\'accept\', \'*/*\'\\n\\tthread=29\\n2025-04-29T18:08:46.944980Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1166\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] request end stream timestamp recorded\\tthread=29\\n2025-04-29T18:08:46.945014Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\\t[Tags: \\"ConnectionId\\":\\"446\\"] current connecting state: false\\tthread=29\\n2025-04-29T18:08:46.945174Z\\tdebug\\tenvoy filter source/extensions/filters/http/alpn/alpn_filter.cc:92\\toverride with 3 ALPNs\\tthread=29\\n2025-04-29T18:08:46.945206Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:527\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] cluster \'outbound|80||global-echo\' match for URL \'/\'\\tthread=29\\n2025-04-29T18:08:46.945308Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:756\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] router decoding headers:\\n\':authority\', \'global-echo\'\\n\':path\', \'/\'\\n\':method\', \'GET\'\\n\':scheme\', \'http\'\\n\'user-agent\', \'curl/8.7.1\'\\n\'accept\', \'*/*\'\\n\'x-forwarded-proto\', \'http\'\\n\'x-request-id\', \'c7b51081-47e1-491b-ba1e-a87bd4608220\'\\n\'x-envoy-decorator-operation\', \'global-echo:80/*\'\\n\'x-envoy-peer-metadata-id\', \'sidecar~10.4.2.55~client-f4cd469d6-wnsrx.curl~curl.svc.cluster.local\'\\n\'x-envoy-peer-metadata\', \'ChoKCkNMVVNURVJfSUQSDBoKS3ViZXJuZXRlcwqIAQoGTEFCRUxTEn4qfAoPCgNhcHASCBoGY2xpZW50CisKH3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLW5hbWUSCBoGY2xpZW50CisKI3NlcnZpY2UuaXN0aW8uaW8vY2Fub25pY2FsLXJldmlzaW9uEgQaAnYxCg8KB3ZlcnNpb24SBBoCdjEKIAoETkFNRRIYGhZjbGllbnQtZjRjZDQ2OWQ2LXduc3J4ChMKCU5BTUVTUEFDRRIGGgRjdXJsCkcKBU9XTkVSEj4aPGt1YmVybmV0ZXM6Ly9hcGlzL2FwcHMvdjEvbmFtZXNwYWNlcy9jdXJsL2RlcGxveW1lbnRzL2NsaWVudAoZCg1XT1JLTE9BRF9OQU1FEggaBmNsaWVudA==\'\\n\'x-envoy-attempt-count\', \'1\'\\n\\tthread=29\\n2025-04-29T18:08:46.945334Z\\tdebug\\tenvoy pool external/envoy/source/common/http/conn_pool_base.cc:78\\tqueueing stream due to no available connections (ready=0 busy=0 connecting=0)\\tthread=29\\n2025-04-29T18:08:46.945342Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:291\\ttrying to create new connection\\tthread=29\\n2025-04-29T18:08:46.945346Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:145\\tcreating a new connection (connecting=0)\\tthread=29\\n2025-04-29T18:08:46.945472Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\\t[Tags: \\"ConnectionId\\":\\"447\\"] current connecting state: true\\tthread=29\\n2025-04-29T18:08:46.945480Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:57\\t[Tags: \\"ConnectionId\\":\\"447\\"] connecting\\tthread=29\\n2025-04-29T18:08:46.945488Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1017\\t[Tags: \\"ConnectionId\\":\\"447\\"] connecting to 10.4.2.45:8080\\tthread=29\\n2025-04-29T18:08:46.945663Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1036\\t[Tags: \\"ConnectionId\\":\\"447\\"] connection in progress\\tthread=29\\n2025-04-29T18:08:46.945707Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:746\\t[Tags: \\"ConnectionId\\":\\"447\\"] connected\\tthread=29\\n2025-04-29T18:08:46.947216Z\\tdebug\\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\\tverify cert failed: SAN matcher\\tthread=29\\n2025-04-29T18:08:46.947284Z\\tdebug\\tenvoy connection external/envoy/source/common/tls/ssl_socket.cc:246\\t[Tags: \\"ConnectionId\\":\\"447\\"] remote address:10.4.2.45:8080,TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:46.947297Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\\t[Tags: \\"ConnectionId\\":\\"447\\"] closing socket: 0\\tthread=29\\n2025-04-29T18:08:46.947340Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:107\\t[Tags: \\"ConnectionId\\":\\"447\\"] disconnect. resetting 0 pending requests\\tthread=29\\n2025-04-29T18:08:46.947378Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:495\\t[Tags: \\"ConnectionId\\":\\"447\\"] client disconnected, failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:46.947403Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:1384\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] upstream reset: reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:46.947452Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:463\\tinvoking 1 idle callback(s) - is_draining_for_deletion_=false\\tthread=29\\n2025-04-29T18:08:46.962109Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:2013\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] performing retry\\tthread=29\\n2025-04-29T18:08:46.962200Z\\tdebug\\tenvoy pool external/envoy/source/common/http/conn_pool_base.cc:78\\tqueueing stream due to no available connections (ready=0 busy=0 connecting=0)\\tthread=29\\n2025-04-29T18:08:46.962208Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:291\\ttrying to create new connection\\tthread=29\\n2025-04-29T18:08:46.962210Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:145\\tcreating a new connection (connecting=0)\\tthread=29\\n2025-04-29T18:08:46.962340Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\\t[Tags: \\"ConnectionId\\":\\"448\\"] current connecting state: true\\tthread=29\\n2025-04-29T18:08:46.962504Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:57\\t[Tags: \\"ConnectionId\\":\\"448\\"] connecting\\tthread=29\\n2025-04-29T18:08:46.962520Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1017\\t[Tags: \\"ConnectionId\\":\\"448\\"] connecting to 10.4.2.45:8080\\tthread=29\\n2025-04-29T18:08:46.962810Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1036\\t[Tags: \\"ConnectionId\\":\\"448\\"] connection in progress\\tthread=29\\n2025-04-29T18:08:46.962854Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:746\\t[Tags: \\"ConnectionId\\":\\"448\\"] connected\\tthread=29\\n2025-04-29T18:08:46.964802Z\\tdebug\\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\\tverify cert failed: SAN matcher\\tthread=29\\n2025-04-29T18:08:46.964857Z\\tdebug\\tenvoy connection external/envoy/source/common/tls/ssl_socket.cc:246\\t[Tags: \\"ConnectionId\\":\\"448\\"] remote address:10.4.2.45:8080,TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:46.964861Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\\t[Tags: \\"ConnectionId\\":\\"448\\"] closing socket: 0\\tthread=29\\n2025-04-29T18:08:46.965002Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:107\\t[Tags: \\"ConnectionId\\":\\"448\\"] disconnect. resetting 0 pending requests\\tthread=29\\n2025-04-29T18:08:46.965017Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:495\\t[Tags: \\"ConnectionId\\":\\"448\\"] client disconnected, failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:46.965098Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:1384\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] upstream reset: reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:46.965141Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:463\\tinvoking 1 idle callback(s) - is_draining_for_deletion_=false\\tthread=29\\n2025-04-29T18:08:47.013428Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:2013\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] performing retry\\tthread=29\\n2025-04-29T18:08:47.013521Z\\tdebug\\tenvoy pool external/envoy/source/common/http/conn_pool_base.cc:78\\tqueueing stream due to no available connections (ready=0 busy=0 connecting=0)\\tthread=29\\n2025-04-29T18:08:47.013525Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:291\\ttrying to create new connection\\tthread=29\\n2025-04-29T18:08:47.013547Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:145\\tcreating a new connection (connecting=0)\\tthread=29\\n2025-04-29T18:08:47.013729Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.h:98\\t[Tags: \\"ConnectionId\\":\\"449\\"] current connecting state: true\\tthread=29\\n2025-04-29T18:08:47.013755Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:57\\t[Tags: \\"ConnectionId\\":\\"449\\"] connecting\\tthread=29\\n2025-04-29T18:08:47.013761Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1017\\t[Tags: \\"ConnectionId\\":\\"449\\"] connecting to 10.4.2.45:8080\\tthread=29\\n2025-04-29T18:08:47.014191Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:1036\\t[Tags: \\"ConnectionId\\":\\"449\\"] connection in progress\\tthread=29\\n2025-04-29T18:08:47.014216Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:746\\t[Tags: \\"ConnectionId\\":\\"449\\"] connected\\tthread=29\\n2025-04-29T18:08:47.015911Z\\tdebug\\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\\tverify cert failed: SAN matcher\\tthread=29\\n2025-04-29T18:08:47.016058Z\\tdebug\\tenvoy connection external/envoy/source/common/tls/ssl_socket.cc:246\\t[Tags: \\"ConnectionId\\":\\"449\\"] remote address:10.4.2.45:8080,TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:47.016062Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\\t[Tags: \\"ConnectionId\\":\\"449\\"] closing socket: 0\\tthread=29\\n2025-04-29T18:08:47.016113Z\\tdebug\\tenvoy client external/envoy/source/common/http/codec_client.cc:107\\t[Tags: \\"ConnectionId\\":\\"449\\"] disconnect. resetting 0 pending requests\\tthread=29\\n2025-04-29T18:08:47.016138Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:495\\t[Tags: \\"ConnectionId\\":\\"449\\"] client disconnected, failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:47.016155Z\\tdebug\\tenvoy router external/envoy/source/common/router/router.cc:1384\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] upstream reset: reset reason: remote connection failure, transport failure reason: TLS_error:|268435581:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end\\tthread=29\\n2025-04-29T18:08:47.016271Z\\tdebug\\tenvoy http external/envoy/source/common/http/filter_manager.cc:1084\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] Sending local reply with details upstream_reset_before_response_started{remote_connection_failure|TLS_error:|268435581:SSL_routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED:TLS_error_end}\\tthread=29\\n2025-04-29T18:08:47.016338Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1878\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] encoding headers via codec (end_stream=false):\\n\':status\', \'503\'\\n\'content-length\', \'239\'\\n\'content-type\', \'text/plain\'\\n\'date\', \'Tue, 29 Apr 2025 18:08:46 GMT\'\\n\'server\', \'envoy\'\\n\\tthread=29\\n2025-04-29T18:08:47.016377Z\\tdebug\\tenvoy http external/envoy/source/common/http/conn_manager_impl.cc:1993\\t[Tags: \\"ConnectionId\\":\\"446\\",\\"StreamId\\":\\"5689876678243589196\\"] Codec completed encoding stream.\\tthread=29\\n2025-04-29T18:08:47.016499Z\\tdebug\\tenvoy pool external/envoy/source/common/conn_pool/conn_pool_base.cc:463\\tinvoking 1 idle callback(s) - is_draining_for_deletion_=false\\tthread=29\\n2025-04-29T18:08:47.017236Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:714\\t[Tags: \\"ConnectionId\\":\\"446\\"] remote close\\tthread=29\\n2025-04-29T18:08:47.017259Z\\tdebug\\tenvoy connection external/envoy/source/common/network/connection_impl.cc:276\\t[Tags: \\"ConnectionId\\":\\"446\\"] closing socket: 0\\tthread=29\\n2025-04-29T18:08:47.017318Z\\tdebug\\tenvoy conn_handler external/envoy/source/common/listener_manager/active_stream_listener_base.cc:136\\t[Tags: \\"ConnectionId\\":\\"446\\"] adding to cleanup list\\tthread=29\\n```\\n\\nAs you can see, the request is being handled by the `outbound|80||global-echo` cluster,\\nand yet the upstream server is still responding with a 503. There is a pertinent\\npart here that hints at what might be going wrong:\\n\\n```\\n2025-04-29T18:08:46.947216Z\\tdebug\\tenvoy connection external/envoy/source/common/tls/cert_validator/default_validator.cc:246\\tverify cert failed: SAN matcher\\tthread=29\\n```\\n\\nour SAN isn\'t getting matched. For anyone not super familiar, a\\nSAN stands for Subject Alternative Name. It is an extension in X.509 certificates (used in TLS/SSL) that allows you to \\nspecify additional identities (such as DNS names, IP addresses, or URIs) that the certificate should be valid for, \\nbeyond the primary Common Name (CN). Istio uses this to verify the spiffeID\\nassociated with the serviceAccount used in the mTLS handshake.\\n\\nLet\'s debug the cluster configuration next in envoy and see if there\'s a difference.\\n\\n(Commands gotten with `istioctl pc cluster -n curl curl-f4cd469d6-wnsrx --fqdn echo.echo.svc.cluster.local -o j\\nson`)\\n\\nDiff between cluster configs of not working (`-`) and not working (`+`)\\n```diff\\n                  \\"defaultValidationContext\\": {\\n                    \\"matchSubjectAltNames\\": [\\n                      {\\n-                       \\"exact\\": \\"spiffe://cluster.local/ns/curl/sa/curl\\"\\n                      }\\n                    ]\\n                  },\\n---\\n                  \\"defaultValidationContext\\": {\\n                    \\"matchSubjectAltNames\\": [\\n                      {\\n+                       \\"exact\\": \\"spiffe://cluster.local/ns/echo/sa/default\\"\\n                      }\\n                    ]\\n                  },\\n```\\n(Note the output has been cleaned to show the pertinent, non-trivial parts)\\n\\nThis is the key. Inside of Istio\'s tls configuration, the configured SAN in\\nthe working configuration is associated with the *destination* service account,\\nwhile in the non-working configuration, it is associated with the *source* service account.\\n\\nAccording to the Istio documentation, we can edit the SAN inside of our ServiceEntry:\\n```yaml\\napiVersion: networking.istio.io/v1\\nkind: ServiceEntry\\nmetadata:\\n  name: global-curl-global-echo\\n  namespace: curl\\nspec:\\n  hosts:\\n    - global-echo\\n  location: MESH_INTERNAL\\n  ports:\\n    - name: http\\n      number: 80\\n      protocol: HTTP\\n      targetPort: 8080\\n  resolution: STATIC\\n  subjectAltNames:\\n    - spiffe://cluster.local/ns/echo/sa/default # <----- Added this\\n  workloadSelector:\\n    labels:\\n      global-service: global-echo\\n```\\n\\nAnd once that\'s applied, we can test our custom host:\\n\\n```bash\\n$ curl global-echo\\ncluster1\\n```\\n\\nIt works! So now we have a custom host routing directly between pod IPs.\\n\\n## Summary\\n\\nIn this post, we learned how to set up a custom host in Istio that routes between\\ntwo different clusters. We learned how to set up a ServiceEntry and WorkloadEntry\\nto route between the two clusters, and how to set up a custom SAN in the ServiceEntry\\nto allow for mTLS communication between the two clusters. We also learned how to\\ndebug the Istio configuration to find out what was going wrong with our mTLS\\nconfiguration, and how to fix it by adding the correct SAN to the ServiceEntry.\\n\\n### Related Issues\\n\\n* https://github.com/istio/istio/issues/37431\\n* https://discuss.istio.io/t/503-between-pod-to-pod-communication-1-5-1/6121\\n* https://stackoverflow.com/questions/62881298/what-is-pod-to-pod-encryption-in-kubernetes-and-how-to-implement-pod-to-pod-enc"}]}}')}}]);